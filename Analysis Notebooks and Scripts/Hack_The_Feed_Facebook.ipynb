{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIkQrKGSaN7M"
      },
      "source": [
        "# Hack The Feed: Insights From Social Media Data\n",
        "\n",
        "### ðŸŽ¯ Project Brief\n",
        "\n",
        "Playhouse Communication is one of Nigeria's leading digital marketing agencies. They combine design and media planning with cutting-edge tech solutions to reimagine what marketing is all about. Their client roster is a mix of global juggernauts and nimble SMEs, each redefining their sectors.\n",
        "\n",
        "We are rolling out the ultimate arena for innovation in data and setting the stage for up and coming data scientists and analysts to showcase their skills, win huge cash prizes, and boost their careers. The \"Hack the Feed\" hackathon is a showdown where data analytics meets creative prowess.\n",
        "\n",
        "Your mission? To decode a treasure trove of social media data for one of our high-profile clients and transform it into game-changing insights.\n",
        "\n",
        "In a rare move, we're handing you the keys to a vault of exclusive social media data to let you dig deep, get creative, and strike gold with actionable insights that could redefine the future of digital marketing.  This isn't just a hackathon; it's your chance to shape the future of digital engagement. ðŸš€\n",
        "\n",
        "Key Deliverables:\n",
        "Participants are expected to:\n",
        "\n",
        "    Create a comprehensive and reproducible report detailing their findings.\n",
        "    Propose actionable recommendations based on the insights.\n",
        "    Create a simple and engaging visualisation of your results & analysis.\n",
        "\n",
        "\n",
        "Evaluation Criteria:\n",
        "Submissions will be evaluated based on the following:\n",
        "\n",
        "    Innovativeness:\n",
        "        Originality and novelty of the insights.\n",
        "    Actionability:\n",
        "        Practicality and feasibility of the recommendations.\n",
        "    Presentation Quality:\n",
        "        Clarity and effectiveness in conveying findings in writing and visual form.\n",
        "    Depth of Analysis:\n",
        "        How thoroughly the data has been explored and understood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWaPwP5-aGSH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Union, List, Literal, Dict, Callable, Tuple, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq49E-ejcZVA"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/Hack The Feed Hackathon'\n",
        "os.chdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YslO81_cP9E"
      },
      "outputs": [],
      "source": [
        "fb_data = pd.read_csv('Post Performance (Stanbic IBTC) January 1, 2013 - July 13, 2023_Facebook.csv', low_memory=False)\n",
        "x_data = pd.read_csv('Post Performance (Stanbic IBTC) January 1, 2013 - July 13, 2023_Twitter.csv', low_memory=False)\n",
        "ig_data = pd.read_csv('Post Performance (Stanbic IBTC) January 1, 2013 - July 13, 2023_Instagram.csv', low_memory=False)\n",
        "ld_data = pd.read_csv('Post Performance (Stanbic IBTC) January 1, 2013 - July 13, 2023_LinkedIn.csv', low_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1az0U4iUpj6B"
      },
      "source": [
        "## Facebook_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7t90-tXfIoO"
      },
      "outputs": [],
      "source": [
        "fb_data.info()\n",
        "\n",
        "# 78 Numerical Columns\n",
        "# 69 Object Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i4OgOzOdAO9"
      },
      "outputs": [],
      "source": [
        "low_cardinality_columns = []\n",
        "high_cardinality_columns = []\n",
        "medium_cardinality_columns = []\n",
        "cat_cols = fb_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in cat_cols:\n",
        "  num_uniq = fb_data[col].nunique()\n",
        "  if num_uniq < 5:\n",
        "    low_cardinality_columns.append(col)\n",
        "  elif num_uniq > 20:\n",
        "    high_cardinality_columns.append(col)\n",
        "  else:\n",
        "    medium_cardinality_columns.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG600_lWpaxW"
      },
      "outputs": [],
      "source": [
        "fb_data[low_cardinality_columns].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WtpY8HkehpQ"
      },
      "source": [
        "### HIGH CARDINALITY COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8Ek-Z-tqAs9"
      },
      "outputs": [],
      "source": [
        "fb_data[high_cardinality_columns[:5]].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CHMbovDq9p-"
      },
      "outputs": [],
      "source": [
        "impression_cols = high_cardinality_columns[5:13]\n",
        "reach_cols = high_cardinality_columns[13:18]\n",
        "fb_data[impression_cols].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnWf_J-Ib7EB"
      },
      "outputs": [],
      "source": [
        "fb_data[reach_cols].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1nWCzBJL0f6"
      },
      "outputs": [],
      "source": [
        "fb_data[impression_cols].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LflW3cX_BzEQ"
      },
      "outputs": [],
      "source": [
        "fb_data[reach_cols].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR295BHcWplF"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach = fb_data\n",
        "\n",
        "for col in cleaned_impressions_reach[impression_cols].columns:\n",
        "  cleaned_impressions_reach = cleaned_impressions_reach[cleaned_impressions_reach[col].notna()]\n",
        "\n",
        "for col in cleaned_impressions_reach[reach_cols].columns:\n",
        "  cleaned_impressions_reach = cleaned_impressions_reach[cleaned_impressions_reach[col].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCtbSiKgDEbO"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[reach_cols] = cleaned_impressions_reach[reach_cols].apply(\n",
        "    lambda row:row.str.replace(',','').astype(\"int\"), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpzZqX7GNuEB"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[impression_cols] = cleaned_impressions_reach[impression_cols].apply(\n",
        "    lambda row:row.str.replace(',','').astype(\"int\"), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoKfa7Ulv219"
      },
      "outputs": [],
      "source": [
        "def get_time_period(hour):\n",
        "    if 6 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "def get_quarter(month):\n",
        "    if 1 <= month <= 3:\n",
        "        return 'Q1'\n",
        "    elif 4 <= month <= 6:\n",
        "        return 'Q2'\n",
        "    elif 7 <= month <= 9:\n",
        "        return 'Q3'\n",
        "    else:\n",
        "        return 'Q4'\n",
        "\n",
        "def get_season(month):\n",
        "    if 3 <= month <= 5:\n",
        "        return 'Spring'\n",
        "    elif 6 <= month <= 8:\n",
        "        return 'Summer'\n",
        "    elif 9 <= month <= 11:\n",
        "        return 'Autumn'\n",
        "    else:\n",
        "        return 'Winter'\n",
        "\n",
        "def get_day_period(day_of_week):\n",
        "    if 0 <= day_of_week <= 4:  # Monday to Friday\n",
        "        return 'Weekday'\n",
        "    else:  # Saturday and Sunday\n",
        "        return 'Weekend'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89RBN60V56vk"
      },
      "outputs": [],
      "source": [
        "holidays = pd.read_excel('NigerianHolidays.xlsx')\n",
        "holidays['month-day'] = pd.to_datetime(holidays.Date).dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSLNGnKiCQhR"
      },
      "outputs": [],
      "source": [
        "holidays = holidays.set_index('month-day')\n",
        "holiday_names = holidays['Name'].to_dict()\n",
        "holiday_types = holidays['Type'].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-dm1QSV3aWS"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach['year'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.year\n",
        "cleaned_impressions_reach['month_name'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.month_name()\n",
        "cleaned_impressions_reach['month'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.month\n",
        "cleaned_impressions_reach['day_name'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.day_name()\n",
        "cleaned_impressions_reach['day'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.day\n",
        "cleaned_impressions_reach['hour'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.hour\n",
        "cleaned_impressions_reach['minute'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.minute\n",
        "cleaned_impressions_reach['day_of_week'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.dayofweek\n",
        "cleaned_impressions_reach['month-day'] = pd.to_datetime(cleaned_impressions_reach.Date).dt.strftime('%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RGfq3F2DTd_"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach['time_period'] = cleaned_impressions_reach['hour'].apply(get_time_period)\n",
        "cleaned_impressions_reach['quarter'] = cleaned_impressions_reach['month'].apply(get_quarter)\n",
        "cleaned_impressions_reach['season'] = cleaned_impressions_reach['month'].apply(get_season)\n",
        "cleaned_impressions_reach['day_period'] = cleaned_impressions_reach['day_of_week'].apply(get_day_period)\n",
        "cleaned_impressions_reach['holiday_names'] = cleaned_impressions_reach['month-day'].map(holiday_names).fillna(\"Regular Day\")\n",
        "cleaned_impressions_reach['holiday_types'] = cleaned_impressions_reach['month-day'].map(holiday_types).fillna(\"Regular Type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab-WNvIMMihw"
      },
      "outputs": [],
      "source": [
        "order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday','Saturday', 'Sunday']\n",
        "cleaned_impressions_reach['day_name'] = pd.Categorical(cleaned_impressions_reach['day_name'], categories=order, ordered=True)\n",
        "\n",
        "\n",
        "\n",
        "order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "cleaned_impressions_reach['time_period'] = pd.Categorical(cleaned_impressions_reach['time_period'], categories=order, ordered=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa2ZZ0jEEDgB"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  df = cleaned_impressions_reach[cleaned_impressions_reach[col]!='Regular Type']\n",
        "  df_yearly = df.groupby(col)[['Post']].count()  # Sum column values for each year\n",
        "  axs[i].plot(df_yearly.index, df_yearly.values, marker='o')\n",
        "\n",
        "  axs[i].set_xlabel(col)\n",
        "  axs[i].set_ylabel('Count')\n",
        "  axs[i].set_title(f'Trend of Posts Across {col}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dljYZjhv-iKa"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[impression_cols].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6J3shnmk_TF"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Impressions'].quantile(0.99) #Posts that had Impression in the Top 1%\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Impressions'] > last_quantile]\n",
        "\n",
        "df.groupby('year')[impression_cols[:2]].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn4bgVuYAkPI"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Impressions'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Impressions'] > last_quantile]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in impression_cols[:2]:\n",
        "    df_rt = df[df[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un3lIi9XiFLS"
      },
      "source": [
        "There doesn't seem to be a correlation between the number of posts in a year and the number of Impressions gotten across the Year. even when there was a sudden increase in the number of posts in the year 2018 - 2022, there was a large decrease in the sum of Impressions in 2018, and a large decrease between the years 2019 - 2022.\n",
        "\n",
        "The number of Impressions across the Time period, seems to decrease linearly from Morning till Evening, this may very well be part of the Facebook Algorithm\n",
        "\n",
        "Thursday and Saturday seem to have the highest number of impressions, compared to wednesday and friday who have the most posts, what makes thursday and saturday so special?\n",
        "\n",
        "The sum of Impressions rises from the first quarter to the second quarter but drops rapidly till the fourth quarter, there seems to be a lot of unorganic impressions after the 2nd quarter\n",
        "\n",
        "Christian Holidays and other religious Holidays have more impressions than others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izAc77_UOyHk"
      },
      "source": [
        "    The Highest Impressions Scores on the Posts were between the years 2018 and 2020, with 2019 having the highest possible values, there was a spike from 2017 which quickly went down but rose up from 2018 till 2019 and then there was a sharp drop from 2019 till 2020.\n",
        "\n",
        "    Having lowest dips in 2018 and 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D8tHiA9XefV"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Non-viral Impressions'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Non-viral Impressions'] > last_quantile]\n",
        "\n",
        "df.groupby('year')[impression_cols[2:4]].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooUlGaydP1Mq"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Non-viral Impressions'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Non-viral Impressions'] > last_quantile]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in impression_cols[2:4]:\n",
        "    df_rt = df[df[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAOUahSddvS_"
      },
      "source": [
        "    Based on the table, here are some creative insights and patterns:\n",
        "\n",
        "    Viral Impressions: The highest number of Viral Impressions, which represents the number of times a post was displayed because it was shared by users and their friends, occurred in 2019. However, there was a significant drop in Viral Impressions in the subsequent years, with the lowest in 2022. This could suggest that the content posted in 2019 resonated particularly well with the audience, prompting them to share it with their friends. The sharp decrease in the following years might indicate a change in content strategy or audience behavior that led to less sharing.\n",
        "\n",
        "    Non-viral Impressions: The highest number of Non-viral Impressions, which represents the number of times any content from your Page entered a userâ€™s screen excluding instances when someoneâ€™s friend likes or follows your Page, engages with a post, shares a photo of your Page, and checks into your Page, occurred in 2019. There was a decrease in Non-viral Impressions in 2020 and 2021, but it increased again in 2023. This could indicate that while fewer users were discovering the content organically in 2020 and 2021, the visibility of the content improved again in 2023.\n",
        "\n",
        "    These trends suggest that while the virality of the content (as measured by Viral Impressions) decreased after 2019, the overall visibility of the content (as measured by Non-viral Impressions) remained relatively high. This could indicate that while fewer users were sharing the content with their friends over time, the content was still reaching a large audience through other means. Itâ€™s important to delve deeper into what might have caused these trends to better inform future content strategies.\n",
        "\n",
        "    For instance, you could explore questions like:\n",
        "\n",
        "    What were the characteristics of posts from 2019 that drove high Viral and Non-viral Impressions?\n",
        "    What changes occurred in 2020 and 2021 that might have led to a decrease in Non-viral Impressions?\n",
        "    What strategies were implemented in 2023 that led to an increase in Non-viral Impressions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2yNaCiGRlZY"
      },
      "source": [
        "    Viral Impressions had a slight drop from 2017-2018 and rose quickly from 2018-2019 but dropped from 2019-2022 but rose from 2022-2023, same thing can be observed from Non-Viral Impressions, a lot of people stopped interacting with the posts from the periods of 2019-2022, why so?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBwGEVVxd66u"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Fan Impressions'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Fan Impressions'] > last_quantile]\n",
        "\n",
        "df.groupby('year')[impression_cols[4:]].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkPdvf00TKVs"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Fan Impressions'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Fan Impressions'] > last_quantile]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in impression_cols[4:]:\n",
        "    df_rt = df[df[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qnTE2hT9uZ"
      },
      "source": [
        "    A sudden decrease from 2017-2018 and then a sharp increan from 2018-2019 and then a sudden decrease from 2019-2022 can be observed from the Fan Impressions, Fan Organic Impressions and Non Fan Impressions, Non Fan Organic Impressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGKsGCHEUhAv"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[reach_cols].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGRVmn7QkwxH"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Reach'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Reach'] > last_quantile]\n",
        "\n",
        "df.groupby('year')[reach_cols[:2]].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0MxDPqXUmu7"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Reach'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Reach'] > last_quantile]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in reach_cols[:2]:\n",
        "    df_rt = df[df[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UVG5IJ0tXs0"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Non-viral Reach'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Non-viral Reach'] > last_quantile]\n",
        "\n",
        "df.groupby('year')[reach_cols[2:4]].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl9uLotOtgY7"
      },
      "source": [
        "      \n",
        "    year \t\tViral Reach \tNon-viral Reach\n",
        "    2017 \t   63985 \t      237526\n",
        "    2018 \t   11830 \t       64202\n",
        "    2019 \t  133035 \t      829701\n",
        "    2020 \t   63622 \t      538092\n",
        "    2021 \t   55165 \t      591436\n",
        "    2022 \t   8962 \t       556239\n",
        "    2023 \t   88301 \t      653503"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9dcOonHVU4K"
      },
      "outputs": [],
      "source": [
        "last_quantile = cleaned_impressions_reach['Non-viral Reach'].quantile(0.99)\n",
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Non-viral Reach'] > last_quantile]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in reach_cols[2:4]:\n",
        "    df_rt = df[df[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jDAVd4n7E8Z"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(25,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in impression_cols:\n",
        "    df_rt = cleaned_impressions_reach[cleaned_impressions_reach[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlpBLyIbw5Vy"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(25,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in reach_cols:\n",
        "    df_rt = cleaned_impressions_reach[cleaned_impressions_reach[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3ATwfaZNZZy"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[cleaned_impressions_reach['Organic Impressions'] > cleaned_impressions_reach['Impressions']][impression_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlUHoSnkUgcT"
      },
      "source": [
        "\n",
        "\n",
        "> When the number of Impressions equals the number of Organic Impressions, it means that all of the times your content was seen were organic impressions. This means that your content was not promoted in any way, but it was still seen by a large number of people.\n",
        "\n",
        "> When the number of Organic Impressions is higher than the number of Impressions, it means that you have some viral impressions. Viral impressions are impressions that come from people sharing, liking, or commenting on your content. This is a good sign, because it means that your content is engaging and people are interested in sharing it with their friends.\n",
        "\n",
        "> When the number of Impressions doesn't equal the number of Organic Impressions, it means that you have some non-organic impressions. Non-organic impressions are impressions that come from promoted content or from people who have seen your content through other means, such as search results or other websites.\n",
        "\n",
        "\n",
        "    If all of your impressions are organic, it means that your content is performing well on its own. However, if you want to reach more people, you could consider promoting your content.\n",
        "\n",
        "    If you have some viral impressions, it means that your content is engaging and people are interested in sharing it with their friends. This is a good sign, and you should continue to create high-quality content that is likely to be shared.\n",
        "\n",
        "    If you have some non-organic impressions, it means that people are seeing your content through other means besides your Facebook Page. This could be a good thing, but it's important to make sure that your content is still relevant and engaging to your target audience.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apyUNwhBLDvU"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[cleaned_impressions_reach['Viral Impressions'] > cleaned_impressions_reach['Non-viral Impressions']][impression_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSgUd88XlmMp"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[cleaned_impressions_reach['Organic Reach'] > cleaned_impressions_reach['Reach']][reach_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVyypi20LlNk"
      },
      "source": [
        "> When the number of Reach equals the number of Organic Reach, it means that all of the times your content was seen were organic impressions. This means that your content was not promoted in any way, but it was still seen by a large number of people.\n",
        "\n",
        "> When the number of Organic Reach is higher than the number of Reach, it means that you have some viral impressions. Viral impressions are impressions that come from people sharing, liking, or commenting on your content. This is a good sign, because it means that your content is engaging and people are interested in sharing it with their friends.\n",
        "\n",
        "> When the number of Reach doesn't equal the number of Organic Reach, it means that you have some non-organic impressions. Non-organic impressions are impressions that come from promoted content or from people who have seen your content through other means, such as search results or other websites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjeY9GQuLoNg"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[cleaned_impressions_reach['Viral Reach'] > cleaned_impressions_reach['Non-viral Reach']][reach_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8R7PdqePAGk"
      },
      "outputs": [],
      "source": [
        "engagement_cols = high_cardinality_columns[18:21]\n",
        "cleaned_impressions_reach['Engagements'] = cleaned_impressions_reach['Engagements'].str.replace(',','').astype('int')\n",
        "cleaned_impressions_reach[engagement_cols + impression_cols[:2] + reach_cols[:2]].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUiBXZ84o44"
      },
      "source": [
        "Engagement Rate(per Impression) = Engagments / Impressions\n",
        "\n",
        "Engagement Rate(per Reach) = Engagements/ Reach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kSFAolv4yu_"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,10))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  df_rt = cleaned_impressions_reach[cleaned_impressions_reach[col]!='Regular Type']\n",
        "  df_yearly = df_rt.groupby(col)['Engagements'].sum()  # Sum column values for each year\n",
        "  axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "  axs[i].set_xlabel('Engagements')\n",
        "  axs[i].set_ylabel('Count')\n",
        "  axs[i].set_title(f'Trend of Engagements Across {col}')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4R3BsS155me"
      },
      "outputs": [],
      "source": [
        "def heatmap(df, length=10, width=5, cmap='rocket'):\n",
        "  heatmap_df = df.corr(numeric_only=True)\n",
        "  fig, ax = plt.subplots(figsize=(length, width))\n",
        "  sns.heatmap(data=heatmap_df, annot=True, cmap=cmap, ax=ax)\n",
        "  # ax.set_title(f'Heatmap of {\" and \".join(list(df.columns))}')\n",
        "  plt.show()\n",
        "\n",
        "corr_matrix = cleaned_impressions_reach[impression_cols[:2]+reach_cols[:2]+engagement_cols]\n",
        "\n",
        "# engagements_corr = corr_matrix['Engagements']\n",
        "\n",
        "# engagements_corr = engagements_corr.sort_values(ascending=False)\n",
        "\n",
        "# engagements_corr\n",
        "\n",
        "heatmap(corr_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y56liRrCT1k"
      },
      "outputs": [],
      "source": [
        "rlc_cols = high_cardinality_columns[21:24]\n",
        "cleaned_impressions_reach[rlc_cols] = cleaned_impressions_reach[rlc_cols].apply(lambda row: row.str.replace(',','').astype('int'),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKPX_y44CoTx"
      },
      "outputs": [],
      "source": [
        "corr_matrix = cleaned_impressions_reach[impression_cols[:2]+reach_cols[:2]+engagement_cols+rlc_cols]\n",
        "heatmap(corr_matrix, length=15, width=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VYrHp0XbGXf"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in rlc_cols:\n",
        "    df_rt = cleaned_impressions_reach[cleaned_impressions_reach[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f75aNeNcRE9"
      },
      "outputs": [],
      "source": [
        "df = cleaned_impressions_reach[cleaned_impressions_reach['Comments'] > cleaned_impressions_reach['Likes']]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in rlc_cols:\n",
        "    df_rt = df[df[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySP_4hSk5gum"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,10))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  df = cleaned_impressions_reach.groupby([col,'Content Type'])[['Content Type']].count().rename(\n",
        "    columns={\"Content Type\":\"Content Count\"}).reset_index()\n",
        "  sns.barplot(x=col, y='Content Count', hue='Content Type', data=df, ax=axs[i])\n",
        "\n",
        "  axs[i].set_xlabel(col)\n",
        "  axs[i].set_ylabel('Count of Content Types')\n",
        "  axs[i].set_title(f'Trend of Content Types Across {col}')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN2hJMwAc0se"
      },
      "outputs": [],
      "source": [
        "click_cols = high_cardinality_columns[24:29]\n",
        "engage_cols = high_cardinality_columns[29:32]\n",
        "unique_click_cols = high_cardinality_columns[32:36]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHcRhcJD2tCE"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[click_cols].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFSepsVNcCyx"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[click_cols[3:]] = cleaned_impressions_reach[click_cols[3:]].fillna(\"0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQQhJKdLc7Gp"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[click_cols[1:]] = cleaned_impressions_reach[click_cols[1:]].apply(\n",
        "    lambda row: row.str.replace(',','').astype('int'), axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIK67lkI4Cgk"
      },
      "outputs": [],
      "source": [
        "opc_pca = cleaned_impressions_reach[cleaned_impressions_reach['Other Post Clicks'] == cleaned_impressions_reach['Post Clicks (All)']]\n",
        "opc_pca[click_cols+['Content Type']].head(3)\n",
        "opc_pca[click_cols+engagement_cols].head(3)\n",
        "opc_pca[click_cols+['Engagements']+rlc_cols].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC5SOfWVim7R"
      },
      "source": [
        "In terms of Facebook metrics, when the **Other Post Clicks** count equals the **Post Clicks (All)** count, and both **Post Photo View Clicks** and **Post Video Play Clicks** are zero, it means that users are interacting with your post, but not with its main content. The interactions are likely on elements categorized as \"Other\", such as clicks on peopleâ€™s names in comments, clicks on the like count, or clicks on the time.\n",
        "\n",
        "The **Click-Through Rate (CTR)** being 0% indicates that although users are seeing your post (it's getting impressions), they are not clicking on the links in your post. This could be because the content of the post is not enticing enough to make users want to learn more, or the links are not relevant to the audience.\n",
        "\n",
        "In terms of how well your post is doing, a high number of **Other Post Clicks** could mean that your post is generating interest and prompting users to interact with it. However, the lack of link clicks and photo/video views suggests that users may not be engaging deeply with your content. They might be reading and interacting with the text and comments, but they're not taking the next step to click on links or view photos/videos.\n",
        "\n",
        "To improve engagement with your posts, you might want to consider making your content more engaging or relevant to your audience. This could involve using more compelling visuals, crafting more engaging text, or sharing links that are highly relevant to your audience. You could also experiment with different types of content to see what resonates most with your audience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeTrWpDgi8xN"
      },
      "outputs": [],
      "source": [
        "opc_pca = cleaned_impressions_reach[cleaned_impressions_reach['Other Post Clicks'] < cleaned_impressions_reach['Post Clicks (All)']]\n",
        "opc_pca[click_cols+['Content Type']].head(3)\n",
        "opc_pca[click_cols+engagement_cols].head(3)\n",
        "opc_pca[click_cols+['Engagements']+rlc_cols].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlp7z3A04d04"
      },
      "outputs": [],
      "source": [
        "heatmap(cleaned_impressions_reach[engagement_cols+rlc_cols+click_cols], length=15, width=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-X772D3a84"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[engage_cols] = cleaned_impressions_reach[engage_cols].apply(\n",
        "    lambda row: row.str.replace(',','').astype('int'), axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AcPKPf9Nrhy"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[engage_cols].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdmS7dX930R7"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in engage_cols:\n",
        "    df_rt = cleaned_impressions_reach[cleaned_impressions_reach[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUZTNtTwQx_r"
      },
      "source": [
        "Comparing the Different Plots, when put alongside the Trend of Reactions, Likes, and Comments, for th Year Column we can see that 2017 and 2020 were the years with peak Reactions, likes, with comments having a peak at 2020 and 2022, but the trends of Engaged Users, Fans and Users talking about the post, have their only peaks at the year 2020, shouldn't more Engaged Users equal more Reactions? Likes and Comments? A sharp drop from the number of Engaged Users, Fans from Afternoon to Evening, Compared to the drop of Reactions, Likes and Comments, Are there Engaged Users but no Reactions, Likes or Comments? The Highest Reactions, and Likes come from Wednesday and Thursday, but Engaged Users, Fans only arrive on Thursday why is that? During the Quarters, the Highest Reactions, Likes, come from the 2nd Quarter with the lowest being in the 3rd Quarter, but here it shows the Engaged Users/Fans dropping from q1 - q4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TB1v5aXTpyZ"
      },
      "source": [
        "Trends of Engaged Users/Fans doesn't seem to follow the Trend of Engagements, why is that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdJgcopAxdI2"
      },
      "outputs": [],
      "source": [
        "heatmap(cleaned_impressions_reach[engagement_cols+rlc_cols+click_cols+engage_cols], length=15, width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MIZv5CjUZ97"
      },
      "source": [
        "As we can see there is a very close correlation with Engaged Users/Engaged Fans and Other Post Clicks, Post Clicks (All) and Post Photo View Clicks,we can come to the conclusion that our Engaged Users/ Engaged Fans are those which add to the Other Post Clicks and Post Clicks (All), those Users/ Fans who are clicking links, opening photos, and making comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCzZi5vDUQPz"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[unique_click_cols] = cleaned_impressions_reach[unique_click_cols].fillna(\"0\")\n",
        "cleaned_impressions_reach[unique_click_cols] = cleaned_impressions_reach[unique_click_cols].apply(\n",
        "    lambda row: row.str.replace(',','').astype('int'), axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVSuHAKxVm4M"
      },
      "outputs": [],
      "source": [
        "cleaned_impressions_reach[unique_click_cols].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k8sRXKRZwZ-"
      },
      "outputs": [],
      "source": [
        "heatmap(cleaned_impressions_reach[unique_click_cols+rlc_cols+engage_cols], length=15, width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf5U_Cpua4xC"
      },
      "source": [
        "We see a closer correlation of Engaged Users, Engaged Fans and Users Talking About This, Users Talking About This --> Unique Reactions , Engaged Users/Fans --> Unique Post Clicks/Unique Other Post Clicks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNimx8rTaH1Q"
      },
      "outputs": [],
      "source": [
        "video_views_cols = high_cardinality_columns[36:42]\n",
        "video_views_2_cols = high_cardinality_columns[42:45]\n",
        "video_views_3_cols = high_cardinality_columns[45:50]\n",
        "autoplay_views_cols = high_cardinality_columns[51:57]\n",
        "unique_video_views_cols = high_cardinality_columns[57:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MApUaHeMbbY0"
      },
      "outputs": [],
      "source": [
        "dropped_video_data = cleaned_impressions_reach.drop(high_cardinality_columns[36:],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avz62tSlerTq"
      },
      "source": [
        "### Medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWJOXWC6b0Xk"
      },
      "outputs": [],
      "source": [
        "medium_cardinality_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uZzowRYhp__"
      },
      "outputs": [],
      "source": [
        "dropped_video_data['Sent by'] = np.where(\n",
        "    dropped_video_data['Sent by']==' ', dropped_video_data['Sent by'].replace(\n",
        "        ' ','Unknown'), dropped_video_data['Sent by'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpTaBqq0kuMW"
      },
      "outputs": [],
      "source": [
        "df_grouped = dropped_video_data.groupby('Sent by')[['Post']].count().sort_values(by=['Post'],ascending=False)\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNnmMdsee6u1"
      },
      "outputs": [],
      "source": [
        "df_grouped = dropped_video_data.groupby('Sent by')[rlc_cols].sum().sort_values(by=['Reactions'],ascending=False)\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIsfejKFfcmb"
      },
      "outputs": [],
      "source": [
        "df_grouped = dropped_video_data.groupby('Sent by')[engage_cols].sum().sort_values(by=['Engaged Users'],ascending=False)\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0XhF8EClYQU"
      },
      "outputs": [],
      "source": [
        "df_grouped = dropped_video_data.groupby('Sent by')[unique_click_cols].sum().sort_values(by=['Unique Post Clicks'],ascending=False)\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svIJZVetlqWI"
      },
      "outputs": [],
      "source": [
        "df_grouped = dropped_video_data.groupby('Sent by')[reach_cols].sum().sort_values(by=['Reach'],ascending=False)\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvqELW0MmsSF"
      },
      "source": [
        "### Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll2CgeUVmedP"
      },
      "outputs": [],
      "source": [
        "num_cols = fb_data.select_dtypes(include=['float64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Tx91-wm1Ob"
      },
      "outputs": [],
      "source": [
        "cols_missing = dropped_video_data[num_cols].isnull().sum().to_dict()\n",
        "for col in cols_missing:\n",
        "  if cols_missing[col] > 0.5 * dropped_video_data.shape[0]:\n",
        "    num_cols=num_cols.drop(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtuA6OT9nTpO"
      },
      "outputs": [],
      "source": [
        "impress_reach_cols = list(num_cols[:5])\n",
        "reactions_cols = list(num_cols[5:10])\n",
        "sfc = list(num_cols[10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PteAUq5Zpu3t"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in reactions_cols:\n",
        "    df_rt = dropped_video_data[cleaned_impressions_reach[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QovWLG4On-Dy"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,15))\n",
        "axs = axs.ravel()\n",
        "\n",
        "plot_cols = ['year','time_period','day_name','day_period','quarter','holiday_types']\n",
        "\n",
        "for i, col in enumerate(plot_cols):\n",
        "  for column in sfc:\n",
        "    df_rt = dropped_video_data[cleaned_impressions_reach[col]!='Regular Type']\n",
        "    df_yearly = df_rt.groupby(col)[column].sum()  # Sum column values for each year\n",
        "    axs[i].plot(df_yearly.index, df_yearly.values, marker='o', label=column)\n",
        "\n",
        "    axs[i].set_xlabel(col)\n",
        "    axs[i].set_ylabel('Count')\n",
        "    axs[i].set_title(f'Trend of Columns Across {col}')\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVsgtB6BsHxy"
      },
      "outputs": [],
      "source": [
        "heatmap(dropped_video_data[reactions_cols+rlc_cols+engagement_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9usvTw1FQv"
      },
      "source": [
        "### Posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRodLXHoC13L"
      },
      "outputs": [],
      "source": [
        "dropped_video_data[['Post']].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKFtVnkyMQgm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from IPython.display import display\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMLDHs2-48CE"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT8Ylh0H4qO5"
      },
      "outputs": [],
      "source": [
        "remove_url=lambda x:re.sub(r'http\\S+','',str(x))\n",
        "to_lower=lambda x: x.lower()\n",
        "remove_puncs= lambda x:x.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "more_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
        "            \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "            \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
        "            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
        "            \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
        "            \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\",\n",
        "            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
        "            \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\",\n",
        "            \"should\", \"now\"]\n",
        "\n",
        "stop_words=set(stopwords.words('english')) #nltk package\n",
        "stop_words.update(more_words)\n",
        "\n",
        "remove_words=lambda x: ' '.join([word for word in x.split() if word not in stop_words]) #.join is from package string\n",
        "\n",
        "def preprocess_text(texts):\n",
        "  texts = texts.apply(remove_url)\n",
        "  texts = texts.apply(to_lower)\n",
        "  texts = texts.apply(remove_puncs)\n",
        "  texts = texts.apply(remove_words)\n",
        "  return texts\n",
        "\n",
        "def clean_text(text):\n",
        "    '''remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "# function to remove emoticons, symbols or flags by their codes\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOMRXgTI6WSC"
      },
      "outputs": [],
      "source": [
        "cleaned_posts = preprocess_text(dropped_video_data['Post'])\n",
        "cleaned_posts = cleaned_posts.apply(lambda x: clean_text(x))\n",
        "cleaned_posts = cleaned_posts.apply(lambda x: remove_emoji(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkSADfEy-uzt"
      },
      "outputs": [],
      "source": [
        "filtered_data = dropped_video_data.copy()\n",
        "filtered_data['Post'] = cleaned_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3KOUbFN8HXR"
      },
      "outputs": [],
      "source": [
        "words_list=[word for line in cleaned_posts for word in line.split()]\n",
        "\n",
        "# creating dataframe and bar graph of most common 50 words with their frequency\n",
        "word_counts=Counter(words_list).most_common(50)\n",
        "word_df=pd.DataFrame(word_counts)\n",
        "word_df.columns=['word','frq']\n",
        "display(word_df.head(5))\n",
        "\n",
        "fig = plt.figure(figsize = (15, 7))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(word_df['word'],word_df['frq'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('word')\n",
        "plt.ylabel('frq')\n",
        "plt.title('Most common words')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5GKclxR_Aq0"
      },
      "source": [
        "We can see the Top 50 words with the most frequency in our dataset, words such as itcanbe which is a hashtag, get, stanbic, ibtc, us, visit, email, call, click and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ztupUcl_TYy"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yqIBlFK_VH4"
      },
      "outputs": [],
      "source": [
        "cut_text = \" \".join(filtered_data['Post'])\n",
        "max_words=100\n",
        "word_cloud = WordCloud(\n",
        "                    background_color='white',\n",
        "                    stopwords=set(stop_words),\n",
        "                    max_words=max_words,\n",
        "                    max_font_size=30,\n",
        "                    scale=5,\n",
        "                    colormap='magma',\n",
        "                    random_state=1).generate(cut_text)\n",
        "fig = plt.figure(1, figsize=(10,10))\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Top '+str(max_words)+' words from Facebook Posts\\n', fontsize=10,color='blue')\n",
        "fig.subplots_adjust(top=2.3)\n",
        "plt.imshow(word_cloud)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCOrqTBx6XXa"
      },
      "source": [
        "Annotations/Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pottgZy-B9tK"
      },
      "outputs": [],
      "source": [
        "sid=SentimentIntensityAnalyzer()\n",
        "ps=lambda x:sid.polarity_scores(x)\n",
        "sentiment_scores=filtered_data['Post'].apply(ps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZabJKLyqCNG-"
      },
      "outputs": [],
      "source": [
        "# create the data frame of negative, neutral, positive and compound polarity scores\n",
        "sentiment_df=pd.DataFrame(data=list(sentiment_scores))\n",
        "labelize=lambda x:'neutral' if x==0 else('positive' if x>0 else 'negative')\n",
        "sentiment_df['sentiment_label']=sentiment_df.compound.apply(labelize)\n",
        "\n",
        "filtered_data = filtered_data.join(sentiment_df['sentiment_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F2edh4gCr7R"
      },
      "outputs": [],
      "source": [
        "filtered_data['sentiment_label'].value_counts().plot(kind='barh',title=\"Bar Plot of Posts Sentiments\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssfXZAGzw3pu"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# spaCy based imports\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40XbonGpfZWL"
      },
      "outputs": [],
      "source": [
        "def topic_modelling(text, n=10):\n",
        "\n",
        "  cvectorizer = CountVectorizer(max_df=0.95, min_df=2,stop_words='english',decode_error='ignore')\n",
        "  lda_model = LatentDirichletAllocation(n_components=8,learning_method='online',max_iter=20,random_state=42)\n",
        "  cvz = cvectorizer.fit_transform(text)\n",
        "  X_topics = lda_model.fit_transform(cvz)\n",
        "  n_top_words = n\n",
        "  topic_summaries = []\n",
        "\n",
        "  topic_word = lda_model.components_  # get the topic words\n",
        "  vocab = cvectorizer.get_feature_names_out()\n",
        "\n",
        "  for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))\n",
        "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))\n",
        "\n",
        "  return cvectorizer, lda_model, topic_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk6I3fc8hRTA"
      },
      "outputs": [],
      "source": [
        "## Get LDA Topics\n",
        "\n",
        "vectorizer, lda_model, topic_summaries = topic_modelling(filtered_data['Post'], n=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbOL7suiivpu"
      },
      "source": [
        "We can create 8 unique categories from this topics\n",
        "\n",
        "1. **Topic 0 - Future Planning**: This topic seems to be about planning for the future, with keywords like \"future\", \"education\", \"children\", and \"time\".\n",
        "\n",
        "2. **Topic 1 - Personal Finance**: This topic appears to be about personal finance and insurance, with keywords like \"wealthwednesday\", \"insurance\", \"share\", and \"gift\".\n",
        "\n",
        "3. **Topic 2 - Investment**: This topic seems to be about investment and savings, with keywords like \"email\", \"invest\", \"account\", and \"savings\".\n",
        "\n",
        "4. **Topic 3 - Banking Services**: This topic appears to be about banking services, with keywords like \"stanbic\", \"ibtc\", \"app\", and \"pension\".\n",
        "\n",
        "5. **Topic 4 - Events & Promotions**: This topic seems to be about events and promotions, with keywords like \"join\", \"win\", and \"register\".\n",
        "\n",
        "6. **Topic 5 - Business & Economy**: This topic appears to be about business and the economy, with keywords like \"business\", \"growth\", and \"nigeria\".\n",
        "\n",
        "7. **Topic 6 - Life & Well-being**: This topic seems to be about life and well-being, with keywords like \"year\", \"insurance\", and \"week\".\n",
        "\n",
        "8. **Topic 7 - Sustainability**: This topic appears to be about sustainability, with keywords like \"sdg\" (Sustainable Development Goals), \"sustainable\", and \"sustainabilitysaturday\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caGI-njKjtnn"
      },
      "outputs": [],
      "source": [
        "topic_labels = ['Future Planning','Personal Finance','Investment','Banking Services','Events & Promotions',\n",
        "                  'Business & Economy','Life & Well-being','Sustainability']\n",
        "\n",
        "def assign_topic_to_text(text, cvectorizer, lda_model, topic_labels):\n",
        "    transformed_text = cvectorizer.transform([text])\n",
        "    topic_distribution = lda_model.transform(transformed_text)\n",
        "    best_topic = np.argmax(topic_distribution[0])\n",
        "\n",
        "    return topic_labels[best_topic]\n",
        "\n",
        "filtered_data['topic_label'] = filtered_data['Post'].apply(\n",
        "    lambda x: assign_topic_to_text(x, vectorizer, lda_model, topic_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJp-sp9dw5Lw"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPdVhbEmFKUa"
      },
      "outputs": [],
      "source": [
        "def named_entity_recognition(text):\n",
        "  doc = nlp(text)\n",
        "  label = [(X.label_) for X in doc.ents]\n",
        "  return label\n",
        "\n",
        "def part_of_speech_tagging(text):\n",
        "  doc = nlp(text)\n",
        "  label = [(X.pos_) for X in doc]\n",
        "  return label\n",
        "\n",
        "filtered_data['ner_label'] = filtered_data['Post'].apply(lambda x:named_entity_recognition(x))\n",
        "filtered_data['pos_label'] = filtered_data['Post'].apply(lambda x:part_of_speech_tagging(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ4w1avJF-UR"
      },
      "outputs": [],
      "source": [
        "filtered_data['ner_count'] = filtered_data['ner_label'].apply(lambda x:len(x))\n",
        "filtered_data['pos_count'] = filtered_data['pos_label'].apply(lambda x:len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g05KX6RaeS3A"
      },
      "outputs": [],
      "source": [
        "ner_tags = ['CARDINAL','DATE','GPE','LOC','MONEY','ORDINAL','ORG','PERSON','TIME']\n",
        "for tag in ner_tags:\n",
        "  filtered_data[tag + '_count'] = filtered_data['ner_label'].apply(lambda x: x.count(tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxNitdwALH2F"
      },
      "outputs": [],
      "source": [
        "pos_tags = ['VERB', 'ADV', 'ADJ', 'NUM', 'NOUN', 'SPACE', 'PROPN']\n",
        "for tag in pos_tags:\n",
        "    filtered_data[tag + '_count'] = filtered_data['pos_label'].apply(lambda x: x.count(tag))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7WZ5FLPBKRz"
      },
      "source": [
        "### Let's Delve in for Deeper Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0UteBq4BZTj"
      },
      "source": [
        "Let's see the distribution of words for each year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwSLFsk3DZJ-"
      },
      "outputs": [],
      "source": [
        "def get_top_n_words(cleaned_posts, ax, n:int=20):\n",
        "    words_list=[word for line in cleaned_posts for word in line.split()]\n",
        "    word_counts=Counter(words_list).most_common(n)\n",
        "    word_df=pd.DataFrame(word_counts)\n",
        "    word_df.columns=['word','frq']\n",
        "\n",
        "    ax.bar(word_df['word'],word_df['frq'])\n",
        "    ax.set_xticklabels(word_df['word'], rotation=90)\n",
        "    ax.set_xlabel('word')\n",
        "    ax.set_ylabel('frq')\n",
        "\n",
        "def word_cloud(texts, ax, n:int=100):\n",
        "    cut_text = \" \".join(texts)\n",
        "    max_words=n\n",
        "    word_cloud = WordCloud(\n",
        "                      background_color='white',\n",
        "                      stopwords=set(stop_words),\n",
        "                      max_words=max_words,\n",
        "                      max_font_size=30,\n",
        "                      scale=1,\n",
        "                      colormap='magma',\n",
        "                      random_state=42).generate(cut_text)\n",
        "    ax.axis('off')\n",
        "    ax.imshow(word_cloud)\n",
        "\n",
        "def sentiment_analyzer(text, ax):\n",
        "    sid=SentimentIntensityAnalyzer()\n",
        "    ps=lambda x:sid.polarity_scores(x)\n",
        "    sentiment_scores=text.apply(ps)\n",
        "\n",
        "    sentiment_df=pd.DataFrame(data=list(sentiment_scores))\n",
        "    labelize=lambda x:'neutral' if x==0 else('positive' if x>0 else 'negative')\n",
        "    sentiment_df['label']=sentiment_df.compound.apply(labelize)\n",
        "\n",
        "    sentiment_df['label'].value_counts().plot(kind='barh', ax=ax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE2xmZmA48cX"
      },
      "outputs": [],
      "source": [
        "def visualize_data(df, column, column_value):\n",
        "    df_year = df[df[column] == column_value]\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    get_top_n_words(df_year['Post'].values, axs[0, 0], n=10)\n",
        "    axs[0, 0].set_title(f\"Top 10 Words in the Top 1% Impression {column_value}\")\n",
        "\n",
        "    word_cloud(df_year['Post'].values, axs[0, 1], n=50)\n",
        "    axs[0, 1].set_title(f\"Word Cloud for Top 50 words in the Top 1% Impression {column_value} period\")\n",
        "\n",
        "    labels = sentiment_analyzer(df_year['Post'], axs[1, 1])\n",
        "    axs[1, 1].set_title(f\"Sentiment Analyzer of Tweets in the Top 1% Impression {column_value} period\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pos_labels(df, column, pos_tags, rows=2, cols=3, plot=False):\n",
        "\n",
        "    pos_cols = [f\"{pos}_count\" for pos in pos_tags]\n",
        "\n",
        "    if plot == False:\n",
        "\n",
        "      display(df.groupby(column)[pos_cols+['pos_count']].sum().sort_values(by=['pos_count'],ascending=False))\n",
        "\n",
        "    else:\n",
        "      df_grouped = df.groupby(column)[pos_cols].sum()\n",
        "\n",
        "      fig, axs = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "      fig.patch.set_facecolor('black')\n",
        "\n",
        "      axs = axs.flatten()\n",
        "\n",
        "      for ax, (year, row) in zip(axs, df_grouped.iterrows()):\n",
        "          ax.pie(row, labels=row.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'], textprops={'color':'#ffffff'})\n",
        "          ax.set_title(f'POS Tag Distribution in {year}', color='white')\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "def plot_ner_labels(df, column, ner_tags, rows=2, cols=3, plot=False):\n",
        "\n",
        "    ner_cols = [f\"{ner}_count\" for ner in ner_tags]\n",
        "\n",
        "    if plot == False:\n",
        "\n",
        "      display(df.groupby(column)[ner_cols+['ner_count']].sum().sort_values(by=['ner_count'],ascending=False))\n",
        "\n",
        "    else:\n",
        "      df_grouped = df.groupby(column)[ner_cols].sum()\n",
        "\n",
        "      fig, axs = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "      fig.patch.set_facecolor('black')\n",
        "\n",
        "      axs = axs.flatten()\n",
        "\n",
        "      for ax, (year, row) in zip(axs, df_grouped.iterrows()):\n",
        "          ax.pie(row, labels=row.index, autopct='%1.1f%%', colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'], textprops={'color':'#ffffff'})\n",
        "          ax.set_title(f'NER Tag Distribution in {year}', color='white')\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59lRT7jw6yUd"
      },
      "outputs": [],
      "source": [
        "def visualize_labels(df, column, column_values, colors, length=15, width=10):\n",
        "\n",
        "  ncols = len(column_values)\n",
        "\n",
        "  fig, axs = plt.subplots(1, ncols, figsize=(length, width))\n",
        "\n",
        "  for i, value in enumerate(column_values):\n",
        "\n",
        "    df[df[column]==value]['topic_label'].value_counts().plot(kind='barh', color=colors[i], ax=axs[i])\n",
        "    axs[i].set_title(f'{value} Topic Labels')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBatUSguDBPo"
      },
      "outputs": [],
      "source": [
        "def barplot_labels(data, plotted_cols, colors, rows=2, cols=4, length=25, width=20):\n",
        "\n",
        "  fig, axs = plt.subplots(nrows=rows, ncols=cols, figsize=(length, width))\n",
        "  axs = axs.ravel()\n",
        "\n",
        "  for i, column in enumerate(plotted_cols):\n",
        "    df_yearly = data.groupby(['topic_label'])[column].mean()\n",
        "    df_yearly = df_yearly.sort_values(ascending=True)\n",
        "    axs[i].barh(df_yearly.index, df_yearly.values, color=colors[i])  # Use color corresponding to column\n",
        "\n",
        "    axs[i].set_xlabel(column)\n",
        "    axs[i].set_ylabel('Topic Label')\n",
        "    axs[i].set_title(f'Distribution of Facebook {column} across Topic Labels', fontsize=10)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_barchart(data, plot_col, length=15, width=10):\n",
        "\n",
        "  df = data.groupby([plot_col, 'topic_label'])[['topic_label']].count().rename(columns={'topic_label':'topic_count'})\n",
        "  df = df.reset_index()\n",
        "\n",
        "  pivot_df = df.pivot(index='topic_label', columns=plot_col, values='topic_count').fillna(0)\n",
        "\n",
        "  # Plotting\n",
        "  plt.figure(figsize=(10,7))\n",
        "  pivot_df.plot(kind='bar', stacked=True, figsize=(10,7))\n",
        "\n",
        "  plt.xlabel('Topic Label')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.title(f'Distribution of Topic Labels Across Each {plot_col}')\n",
        "  plt.legend(title=plot_col)\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "WR0TtBT4rk0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['skyblue', 'olive', 'gold', 'purple', 'red', 'green', 'orange', 'brown']"
      ],
      "metadata": {
        "id": "333-3qhcwVKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK3b45GnZeEj"
      },
      "source": [
        "#### Distribution of Whole Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcT2cgCfWzIj"
      },
      "outputs": [],
      "source": [
        "filtered_data['topic_label'].value_counts().plot(kind='bar',title=\"Distribution of Topic Models\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isur8v9gZrse"
      },
      "outputs": [],
      "source": [
        "pos_cols = [f\"{pos}_count\" for pos in pos_tags]\n",
        "ner_cols = [f\"{ner}_count\" for ner in ner_tags]\n",
        "\n",
        "filtered_data.groupby(['topic_label'])[pos_cols+['pos_count']].mean().sort_values(by=['pos_count'],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3u6Zopo33F"
      },
      "outputs": [],
      "source": [
        "filtered_data.groupby(['topic_label'])[ner_cols+['ner_count']].mean().sort_values(by=['ner_count'],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYc0DXhAP9AS"
      },
      "outputs": [],
      "source": [
        "barplot_labels(filtered_data, impression_cols, colors,length=20,width=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRqnd_tNQspE"
      },
      "outputs": [],
      "source": [
        "barplot_labels(filtered_data, reach_cols, colors, rows=2, cols=3,length=15,width=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data['Engagement Rate (per Impression)'] = filtered_data['Engagements'] / filtered_data['Impressions'] * 100\n",
        "filtered_data['Engagement Rate (per Reach)'] = filtered_data['Engagements'] / filtered_data['Reach'] * 100"
      ],
      "metadata": {
        "id": "MzZGusRbV3te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data[engagement_cols] = filtered_data[engagement_cols].replace([np.inf, -np.inf, np.nan], 0)\n"
      ],
      "metadata": {
        "id": "J50v7joHXRsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, engagement_cols, colors, rows=1, cols=3,length=15, width=10)"
      ],
      "metadata": {
        "id": "J6ab3JRHVg31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, rlc_cols, colors, rows=1, cols=3,length=15, width=10)"
      ],
      "metadata": {
        "id": "pNhqWUfRdAPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, click_cols[1:4], colors, rows=1, cols=3, length=15, width=10)"
      ],
      "metadata": {
        "id": "AlcPIpK5eRlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, engage_cols, colors, rows=1, cols=3, length=15, width=10)"
      ],
      "metadata": {
        "id": "c1PZsS5-jb9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, unique_click_cols, colors, rows=2, cols=2, length=15, width=10)"
      ],
      "metadata": {
        "id": "AINBIT0gmh7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, reactions_cols, colors, rows=2, cols=3, length=15, width=10)"
      ],
      "metadata": {
        "id": "J_W7fqowox7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barplot_labels(filtered_data, sfc, colors, rows=2, cols=4, length=25, width=10)"
      ],
      "metadata": {
        "id": "37Ltmvxgpg-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_barchart(filtered_data, \"year\")"
      ],
      "metadata": {
        "id": "gIEojccnuj6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_barchart(filtered_data, \"time_period\")"
      ],
      "metadata": {
        "id": "NEfTbtlGuxCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_barchart(filtered_data, \"day_name\")"
      ],
      "metadata": {
        "id": "4oZzMsXYwW-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_barchart(filtered_data, \"day_period\")"
      ],
      "metadata": {
        "id": "clfIUpTfwXJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_barchart(filtered_data, \"quarter\")"
      ],
      "metadata": {
        "id": "2mBGdzD8wXzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrt = filtered_data[filtered_data['holiday_types']!='Regular Type']\n",
        "stack_barchart(wrt, \"holiday_types\")"
      ],
      "metadata": {
        "id": "LhYeRpSruyoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMSb9D-Ra-yZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6_AOJofxYN_"
      },
      "source": [
        "#### Top 1% Impressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_D7TVTkBeeY"
      },
      "outputs": [],
      "source": [
        "last_quantile = filtered_data['Impressions'].quantile(0.99) #Posts that had Impression in the Top 1%\n",
        "df = filtered_data[cleaned_impressions_reach['Impressions'] > last_quantile]\n",
        "\n",
        "visualize_data(df, \"year\", 2019)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjSWAjElqJe8"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"year\", 2018)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duMxaO7QrUdX"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"year\", 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9kqcjjr6b-A"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"year\", [2018, 2019, 2022], colors, length=15, width=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVWpW2lDHyvd"
      },
      "outputs": [],
      "source": [
        "plot_pos_labels(df, \"year\",  pos_tags, plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ner_labels(df, \"year\",  ner_tags, plot=False)"
      ],
      "metadata": {
        "id": "xhvOIG5fXene"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sOy55sPwWRS"
      },
      "source": [
        "Our Insights are finally coming together as we can see and have a clearer explanation now, Posts that were in the top 1% Impression, had the highest Verb count, Adverb count, Adjectives count, Noun Count, Using more parts of Speech in our posts helps us to generate more insights, we can also see compared to the others 2019 had the highest number of topics being discussed. When our posts are distributed it helps to generate more insights, the year 2019 also had the most Named Entities Recognized, so we had Important Dates, Organizations mentioned in our Posts which gathered more Impressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voKb1F7ZuTsq"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"time_period\", \"Morning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsR_w-gDx1cJ"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"time_period\", \"Afternoon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMbrqS1syMNS"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"time_period\", \"Evening\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIxoOGkSyp_q"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"time_period\", [\"Morning\", \"Afternoon\", \"Evening\"], colors, length=15, width=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkLHmDXUy4hO"
      },
      "outputs": [],
      "source": [
        "plot_pos_labels(df, \"time_period\", pos_tags, rows=2, cols=2, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTWHZW9oz5Dy"
      },
      "source": [
        "This proves our theory behind the years with the highest Impression as we can see the same occuring here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOeLmyRTziZN"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Thursday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2vuUgt40N-a"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Friday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_8AVKCLB8KT"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Saturday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEeQ8dDUB8ZK"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Sunday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHMWH24aCEmr"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"day_name\", [\"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], colors, length=25, width=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONR_4eEcCHaz"
      },
      "outputs": [],
      "source": [
        "plot_pos_labels(df, \"day_name\", pos_tags, rows=2, cols=3, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbNCCrhu3QVY"
      },
      "source": [
        "\n",
        "\n",
        "This proves our theory behind the years with the highest Impression as we can see the same occuring here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1t1Hzn52p2A"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"quarter\", \"Q1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mjjBdLXCbXm"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"quarter\", \"Q2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdIO54idChRz"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"quarter\", [\"Q1\",\"Q2\"],colors, length=15, width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XRMTHBiDEAe"
      },
      "source": [
        "#### Fan 1% Impressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZVXfLb0C69r"
      },
      "outputs": [],
      "source": [
        "last_quantile = filtered_data['Fan Impressions'].quantile(0.99) #Posts that had Impression in the Top 1%\n",
        "df = filtered_data[cleaned_impressions_reach['Fan Impressions'] > last_quantile]\n",
        "\n",
        "visualize_data(df, \"year\", 2019)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMKDUYFFDuur"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"year\", 2018)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwKZzz56Dy93"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"year\", 2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAEavsWID0x-"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"year\", [2018, 2019, 2022],colors, length=15, width=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4jjb3jtEMWl"
      },
      "outputs": [],
      "source": [
        "plot_pos_labels(df, \"quarter\",  pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV0thxkAEODv"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Tuesday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaIYGXmrEffF"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Thursday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGbmFhYGEftI"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"day_name\", \"Saturday\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls7p6Y3cEp42"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"day_name\", [\"Tuesday\", \"Thursday\", \"Saturday\"], colors, length=25, width=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxCqOhlMFENp"
      },
      "outputs": [],
      "source": [
        "plot_pos_labels(df, \"day_name\",  pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtHm596BFc2A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24JxTj0IFxSL"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"quarter\", \"Q1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfku_KyZF6g9"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"quarter\", \"Q2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d0iKMfxF6t6"
      },
      "outputs": [],
      "source": [
        "visualize_data(df, \"quarter\", \"Q3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eM1_kP9F-En"
      },
      "outputs": [],
      "source": [
        "visualize_labels(df, \"quarter\", [\"Q1\", \"Q2\", \"Q3\"],colors,  length=25, width=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T7PvDuVGF3R"
      },
      "outputs": [],
      "source": [
        "plot_pos_labels(df, \"quarter\",  pos_tags)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3WtpY8HkehpQ",
        "Avz62tSlerTq",
        "RvqELW0MmsSF",
        "fy9usvTw1FQv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}